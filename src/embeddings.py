"""
Multi-modal embedding generation for text and images.
"""
from typing import List, Union
import os
import numpy as np
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()


class EmbeddingGenerator:
    """
    Generate embeddings for text and multi-modal content.
    """

    def __init__(self, model_name: str = None, api_key: str = None):
        """
        Initialize the embedding generator.

        Args:
            model_name: Name of the embedding model
            api_key: OpenAI API key
        """
        self.model_name = model_name or os.getenv('EMBEDDING_MODEL', 'text-embedding-3-small')
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        self.client = OpenAI(api_key=self.api_key)
        self.dimension = 1536  # Default dimension for text-embedding-3-small

    def embed_text(self, text: str) -> np.ndarray:
        """
        Generate embedding for a single text.

        Args:
            text: Input text

        Returns:
            Embedding vector as numpy array
        """
        if not text or len(text.strip()) == 0:
            return np.zeros(self.dimension)

        response = self.client.embeddings.create(
            input=text,
            model=self.model_name
        )

        embedding = response.data[0].embedding
        return np.array(embedding)

    def embed_texts(self, texts: List[str]) -> List[np.ndarray]:
        """
        Generate embeddings for multiple texts in batch.

        Args:
            texts: List of input texts

        Returns:
            List of embedding vectors
        """
        if not texts:
            return []

        # Filter out empty texts
        filtered_texts = [t if t and t.strip() else " " for t in texts]

        response = self.client.embeddings.create(
            input=filtered_texts,
            model=self.model_name
        )

        embeddings = [np.array(data.embedding) for data in response.data]
        return embeddings

    def embed_query(self, query: str) -> np.ndarray:
        """
        Generate embedding for a search query.

        Args:
            query: Search query text

        Returns:
            Query embedding vector
        """
        return self.embed_text(query)

    def combine_embeddings(
        self,
        text_embedding: np.ndarray,
        image_embedding: np.ndarray,
        text_weight: float = 0.7,
        image_weight: float = 0.3
    ) -> np.ndarray:
        """
        Combine text and image embeddings with weighted average.

        Args:
            text_embedding: Text embedding vector
            image_embedding: Image embedding vector
            text_weight: Weight for text embedding (default 0.7)
            image_weight: Weight for image embedding (default 0.3)

        Returns:
            Combined embedding vector
        """
        combined = (text_weight * text_embedding + image_weight * image_embedding)
        # Normalize
        norm = np.linalg.norm(combined)
        if norm > 0:
            combined = combined / norm
        return combined

    def embed_document_chunk(
        self,
        text: str,
        image_description: str = None
    ) -> np.ndarray:
        """
        Generate embedding for a document chunk with optional image context.

        Args:
            text: Document text
            image_description: Optional image description from vision model

        Returns:
            Document chunk embedding
        """
        if image_description:
            # Combine text with image description for richer embedding
            combined_text = f"{text}\n\nImage content: {image_description}"
            return self.embed_text(combined_text)
        else:
            return self.embed_text(text)

    def get_embedding_dimension(self) -> int:
        """
        Get the dimension of embeddings generated by this model.

        Returns:
            Embedding dimension
        """
        return self.dimension
